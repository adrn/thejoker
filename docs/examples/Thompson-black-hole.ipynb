{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the black hole discovery in Thompson et al. 2019\n",
    "\n",
    "In this science demo tutorial, we will reproduce the results in [Thompson et al. 2019](https://ui.adsabs.harvard.edu/abs/2019Sci...366..637T/abstract), who found and followed-up a candidate stellar-mass black hole companion to a giant star in the Milky Way. We will first use *The Joker* to constrain the orbit of the system using the TRES follow-up radial velocity data released in their paper and show that we get consistent period and companion mass constraints from modeling these data. We will then do a joint analysis of the TRES and APOGEE data for this source by simultaneously fitting for and marginalizing over an unknown constant velocity offset between the two surveys.\n",
    "\n",
    "A bunch of imports we will need later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import astropy.units as u\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import thejoker as tj\n",
    "import thejoker.units as xu\n",
    "from astropy.io import ascii\n",
    "from astropy.time import Time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a random number generator to ensure reproducibility\n",
    "seed = 42\n",
    "rnd = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will start by loading data, copy-pasted from Table S2 in Thompson et al. 2019):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tres_tbl = ascii.read(\n",
    "    \"\"\"8006.97517 0.000 0.075\n",
    "    8023.98151 -43.313 0.075\n",
    "    8039.89955 -27.963 0.045\n",
    "    8051.98423 10.928 0.118\n",
    "    8070.99556 43.782 0.075\n",
    "    8099.80651 -30.033 0.054\n",
    "    8106.91698 -42.872 0.135\n",
    "    8112.81800 -44.863 0.088\n",
    "    8123.79627 -25.810 0.115\n",
    "    8136.59960 15.691 0.146\n",
    "    8143.78352 34.281 0.087\"\"\",\n",
    "    names=[\"HJD\", \"rv\", \"rv_err\"],\n",
    ")\n",
    "tres_tbl[\"rv\"].unit = u.km / u.s\n",
    "tres_tbl[\"rv_err\"].unit = u.km / u.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apogee_tbl = ascii.read(\n",
    "    \"\"\"6204.95544 -37.417 0.011\n",
    "    6229.92499 34.846 0.010\n",
    "    6233.87715 42.567 0.010\"\"\",\n",
    "    names=[\"HJD\", \"rv\", \"rv_err\"],\n",
    ")\n",
    "apogee_tbl[\"rv\"].unit = u.km / u.s\n",
    "apogee_tbl[\"rv_err\"].unit = u.km / u.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tres_data = tj.RVData(\n",
    "    t=Time(tres_tbl[\"HJD\"] + 2450000, format=\"jd\", scale=\"tcb\"),\n",
    "    rv=u.Quantity(tres_tbl[\"rv\"]),\n",
    "    rv_err=u.Quantity(tres_tbl[\"rv_err\"]),\n",
    ")\n",
    "\n",
    "apogee_data = tj.RVData(\n",
    "    t=Time(apogee_tbl[\"HJD\"] + 2450000, format=\"jd\", scale=\"tcb\"),\n",
    "    rv=u.Quantity(apogee_tbl[\"rv\"]),\n",
    "    rv_err=u.Quantity(apogee_tbl[\"rv_err\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the data from these two instruments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, name in zip([tres_data, apogee_data], [\"TRES\", \"APOGEE\"]):\n",
    "    d.plot(color=None, label=name)\n",
    "plt.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run The Joker with just the TRES data\n",
    "\n",
    "The two data sets are separated by a large gap in observations between the end of APOGEE and the start of the RV follow-up with TRES. Since there are more observations with TRES, we will start by running *The Joker* with just data from TRES before using all of the data. Let's plot the TRES data alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tres_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty clear that there is a periodic signal in the data, with a period between 10s to ~100 days (from eyeballing the plot above), so this limits the range of periods we need to sample over with *The Joker* below. The reported uncertainties on the individual RV measurements (plotted above, I swear) are all very small (typically smaller than the markers). So, we may want to allow for the fact that these could be under-estimated. With *The Joker*, we support this by accepting an additional nonlinear parameter, `s`, that specifies a global, extra uncertainty that is added in quadrature to the data uncertainties while running the sampler. That is, the uncertainties used for computing the likelihood in *The Joker* are computed as:\n",
    "$$\n",
    "\\sigma_n = \\sqrt{\\sigma_{n,0}^2 + s^2}\n",
    "$$\n",
    "where $\\sigma_{n,0}$ are the values reported for each $n$ data point in the tables above. We'll use a log-normal prior on this extra error term, but will otherwise use the default prior form for The Joker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Allow extra error to account for under-estimated error bars\n",
    "    s = xu.with_unit(pm.Lognormal(\"s\", -2, 1), u.km / u.s)\n",
    "\n",
    "    prior = tj.JokerPrior.default(\n",
    "        P_min=16 * u.day,\n",
    "        P_max=128 * u.day,  # Range of periods to consider\n",
    "        sigma_K0=30 * u.km / u.s,\n",
    "        P0=1 * u.year,  # scale of the prior on semiamplitude, K\n",
    "        sigma_v=25 * u.km / u.s,  # std dev of the prior on the systemic velocity, v0\n",
    "        s=s,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the prior set up, we can now generate prior samples, and run the rejection sampling step of *The Joker*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a large number of prior samples:\n",
    "prior_samples = prior.sample(size=1_000_000, rng=rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rejection sampling with The Joker:\n",
    "joker = tj.TheJoker(prior, rng=rnd)\n",
    "samples = joker.rejection_sample(tres_data, prior_samples, max_posterior_samples=256)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 1 sample is returned from the rejection sampling step - let's see how well it matches the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tj.plot_rv_curves(samples, data=tres_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the values of the sample that was returned, and compare that to the values reported in Thompson et al. 2019, included below for convenience:\n",
    "$$\n",
    "P = 83.205 \\pm 0.064\\\\\n",
    "e = 0.00476 \\pm 0.00255\\\\\n",
    "K = 44.615 \\pm 0.123\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.tbl[\"P\", \"e\", \"K\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already these look very consistent with the values inferred in the paper! \n",
    "\n",
    "Let's now also plot the data phase-folded on the period returned in the one sample we got from *The Joker*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tres_data.plot(phase_fold=samples[0][\"P\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, since the data are very constraining, we could use this one *Joker* sample to initialize standard MCMC to generate posterior samplings in the orbital parameters for this system. We will do that below, but first let's see how things look if we include both TRES *and* APOGEE data in our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run The Joker with TRES+APOGEE data\n",
    "\n",
    "One of the challenges with incorporating data from the two surveys is that they were taken with two different spectrographs, and there could be instrumental offsets that manifest as shifts in the absolute radial velocities measured between the two instruments. *The Joker* now supports simultaneously sampling over additional parameters that represent instrumental or calibratrion offsets, so let's take a look at how to run *The Joker* in this mode. \n",
    "\n",
    "To start, we can pack the two datasets into a single list that contains data from both surveys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [apogee_data, tres_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run anything, let's try phase-folding both datasets on the period value we got from running on the TRES data alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tres_data.plot(color=None, phase_fold_period=np.mean(samples[\"P\"]))\n",
    "apogee_data.plot(color=None, phase_fold_period=np.mean(samples[\"P\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, but the period is clearly slightly off and there seems to be a constant velocity offset between the two surveys, given that the APOGEE RV points don't seem to lie in the RV curve. So, let's now try running *The Joker* on the joined dataset!\n",
    "\n",
    "To allow for an unknown constant velocity offset between TRES and APOGEE, we have to define a new parameter for this offset and specify a prior. We'll put a Gaussian prior on this offset parameter (named `dv0_1` below), with a mean of 0 and a standard deviation of 10 km/s, because it doesn't look like the surveys have a huge offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # The parameter that represents the constant velocity offset between\n",
    "    # APOGEE and TRES:\n",
    "    dv0_1 = xu.with_unit(pm.Normal(\"dv0_1\", 0, 5.0), u.km / u.s)\n",
    "\n",
    "    # The same extra uncertainty parameter as previously defined\n",
    "    s = xu.with_unit(pm.Lognormal(\"s\", -2, 1), u.km / u.s)\n",
    "\n",
    "    # We can restrict the prior on prior now, using the above\n",
    "    prior_joint = tj.JokerPrior.default(\n",
    "        # P_min=16*u.day, P_max=128*u.day,\n",
    "        P_min=75 * u.day,\n",
    "        P_max=90 * u.day,\n",
    "        sigma_K0=30 * u.km / u.s,\n",
    "        P0=1 * u.year,\n",
    "        sigma_v=25 * u.km / u.s,\n",
    "        v0_offsets=[dv0_1],\n",
    "        s=s,\n",
    "    )\n",
    "\n",
    "prior_samples_joint = prior_joint.sample(size=1_000_000, rng=rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rejection sampling with The Joker:\n",
    "joker_joint = tj.TheJoker(prior_joint, rng=rnd)\n",
    "samples_joint = joker_joint.rejection_sample(\n",
    "    data, prior_samples_joint, max_posterior_samples=256\n",
    ")\n",
    "samples_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we again only get one sample back from *The Joker*, because these data are so constraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tj.plot_rv_curves(samples_joint, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fire up standard MCMC, using the one *Joker* sample to initialize. We will use the NUTS sampler in `pymc3` to run here. When running MCMC to model radial velocities with Keplerian orbits, it is typically important to think about the parametrization. There are several angle parameters in the two-body problem (e.g., argument of pericenter, phase, inclination, etc.) that can be especially hard to sample over na√Øvely. Here, for running MCMC, we will instead sample over $M_0 - \\omega, \\omega$ instead of $M_0, \\omega$, and we will define these angles as `pymc3_ext.distributions.Angle` distributions, which [internally transform and sample in](https://exoplanet.dfm.io/en/stable/user/api/#exoplanet.distributions.Angle) $\\cos{x}, \\sin{x}$ instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc_ext.distributions import angle\n",
    "\n",
    "with pm.Model():\n",
    "    # See note above: when running MCMC, we will sample in the parameters\n",
    "    # (M0 - omega, omega) instead of (M0, omega)\n",
    "    M0_m_omega = xu.with_unit(angle(\"M0_m_omega\"), u.radian)\n",
    "    omega = xu.with_unit(angle(\"omega\"), u.radian)\n",
    "    # M0 = xu.with_unit(angle('M0'), u.radian)\n",
    "    M0 = xu.with_unit(pm.Deterministic(\"M0\", M0_m_omega + omega), u.radian)\n",
    "\n",
    "    # The same offset and extra uncertainty parameters as above:\n",
    "    dv0_1 = xu.with_unit(pm.Normal(\"dv0_1\", 0, 5.0), u.km / u.s)\n",
    "    s = xu.with_unit(pm.Lognormal(\"s\", -2, 0.5), u.km / u.s)\n",
    "\n",
    "    prior_mcmc = tj.JokerPrior.default(\n",
    "        P_min=16 * u.day,\n",
    "        P_max=128 * u.day,\n",
    "        sigma_K0=30 * u.km / u.s,\n",
    "        P0=1 * u.year,\n",
    "        sigma_v=25 * u.km / u.s,\n",
    "        v0_offsets=[dv0_1],\n",
    "        s=s,\n",
    "        pars={\"M0\": M0, \"omega\": omega},\n",
    "    )\n",
    "\n",
    "    joker_mcmc = tj.TheJoker(prior_mcmc, rng=rnd)\n",
    "    mcmc_init = joker_mcmc.setup_mcmc(data, samples_joint)\n",
    "\n",
    "    trace = pm.sample(\n",
    "        tune=1000, draws=500, start=mcmc_init, random_seed=seed, cores=1, chains=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `pymc3` to look at some statistics of the MC chains to assess convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace, var_names=prior_mcmc.par_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then transform the MCMC samples back into a `JokerSamples` instance so we can manipulate and visualize the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_samples = tj.JokerSamples.from_inference_data(prior_joint, trace, data=data)\n",
    "mcmc_samples = mcmc_samples.wrap_K()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can make a [corner](https://corner.readthedocs.io/en/latest/) plot of the orbital parameters (note the strong degenceracy between `M0` and `omega`! But also note that we don't sample in these parameters explicitly, so this shouldn't affect convergence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mcmc_samples.tbl.to_pandas()\n",
    "_ = corner.corner(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the median MCMC sample to fold the data and plot residuals relative to our inferred RV model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(6, 8), sharex=True)\n",
    "\n",
    "_ = tj.plot_phase_fold(mcmc_samples.median_period(), data, ax=axes[0], add_labels=False)\n",
    "_ = tj.plot_phase_fold(mcmc_samples.median_period(), data, ax=axes[1], residual=True)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylabel(f\"RV [{apogee_data.rv.unit:latex_inline}]\")\n",
    "\n",
    "axes[1].axhline(0, zorder=-10, color=\"tab:green\", alpha=0.5)\n",
    "axes[1].set_ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's convert our orbit samples into binary mass function, $f(M)$, values to compare with one of the main conclusions of the Thompson et al. paper. We can do this by first converting the samples to `KeplerOrbit` objects, and then using the `.m_f` attribute to get the binary mass function values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfs = u.Quantity(\n",
    "    [mcmc_samples.get_orbit(i).m_f for i in rnd.choice(len(mcmc_samples), 1024)]\n",
    ")\n",
    "plt.hist(mfs.to_value(u.Msun), bins=32)\n",
    "plt.xlabel(rf\"$f(M)$ [{u.Msun:latex_inline}]\")\n",
    "# Values from Thompson et al., showing 1-sigma region\n",
    "plt.axvline(0.766, zorder=100, color=\"tab:orange\")\n",
    "plt.axvspan(\n",
    "    0.766 - 0.00637, 0.766 + 0.00637, zorder=10, color=\"tab:orange\", alpha=0.4, lw=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, using both the APOGEE and TRES data, we confirm the results from the paper, and find that the binary mass function value suggests a large mass companion. A success for reproducible science!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
