\documentclass[12pt, preprint]{aastex}

% to-do list
% ----------
% - zeroth draft

% style notes
% -----------
% - This file generates by Makefile; don't be typing ``pdflatex'' or some bullshit.
% - Line break between sentences to make the git diffs readable.
% - Simple Monte Carlo gets a capital S to indicate that it is a defined thing.
% - Use \, as a multiply operator.
% - Reserve () for function arguments; use [] or {} for outer shit.
% - Always prior pdf or posterior pdf, never prior or posterior.

\include{gitstuff}

% fix aastex
\setlength{\parindent}{1.4em}
\linespread{1.15} % close to 10/13 spacing in ``manuscript''
\setlength{\parskip}{0ex}
\makeatletter % you know you are living your life wrong when you need to do this
\def\revtex@pageid{%
 \xdef\@thefnmark{\null}%
 \@footnotetext{%
  This \revtex@genre\space was prepared with the
  \revtex@org\space \LaTeX\ macros v\revtex@ver, with modifications by David~W.~Hogg.%
 }%
}%
\def\abstractname{Abstract}%
\makeatother
\sloppy\sloppypar\frenchspacing

% packages
\usepackage{amsmath}
\usepackage[backref,breaklinks,colorlinks,citecolor=black]{hyperref}
\usepackage[all]{hypcap}
\renewcommand*{\backref}[1]{[#1]}

% define macros for text
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\apogee}{\project{\acronym{APOGEE}}}
\newcommand{\samplername}{\project{The~Joker}}
\newcommand{\emcee}{\project{emcee}}
\newcommand{\dr}{\acronym{DR13}}

% define macros for math
\newcommand{\meterspersecond}{\mathrm{m\,s^{-1}}}
\newcommand{\asini}{\ensuremath{a\,\sin i}}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\inv}[1]{{#1}^{-1}}
\newcommand{\msun}{\mathrm{M}_\odot}
\newcommand{\bs}[1]{\boldsymbol{#1}}

\begin{document}

\title{\samplername: A custom Monte Carlo sampler \\
  for binary-star and exoplanet radial velocity data}
\author{Adrian~M.~Price-Whelan\altaffilmark{\pu,\adrn},
        David~W.~Hogg\altaffilmark{\ccpp,\mpia},
        Daniel~Foreman-Mackey\altaffilmark{\uw,\sagan},
        Hans-Walter~Rix\altaffilmark{\mpia}
}

% Affiliations
\newcommand{\pu}{1}
\newcommand{\adrn}{2}
\newcommand{\ccpp}{3}
\newcommand{\mpia}{4}
\newcommand{\uw}{5}
\newcommand{\sagan}{6}

\altaffiltext{\pu}{Department of Astrophysical Sciences,
                   Princeton University, Princeton, NJ 08544, USA}
\altaffiltext{\adrn}{To whom correspondence should be addressed:
                     \texttt{adrn@princeton.edu}}
\altaffiltext{\ccpp}{Center for Cosmology and Particle Physics,
                     Department of Physics,
                     New York University, 4 Washington Place,
                     New York, NY 10003, USA}
\altaffiltext{\mpia}{Max-Planck-Institut f\"ur Astronomie,
                     K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{\uw}{Astronomy Department, University of Washington,
                   Seattle, WA 98195, USA}
\altaffiltext{\sagan}{Sagan Fellow}

\begin{abstract}
% Context
Given sparse radial-velocity measurements of a star, there are often
many qualitatively different stellar or exoplanet companion orbit
models that are consistent with the data.
The consequent multimodality of the likelihood function leads to
extremely challenging search, optimization, and MCMC posterior
sampling in the space of orbital parameters.
% Aims
Here we create a custom-built Monte Carlo sampler that can produce a
posterior sampling for orbital parameters given even small numbers of
noisy radial-velocity measurements (and hence a very complex likelihood
function).
The goal is to obtain provably correct samplings in the space of
orbital parameters.
% Methods
We build a variant of Simple Monte Carlo sampling, in which we densely
sample the non-linear orbital parameters, and perform rejection
sampling using a marginalized likelihood, marginalizing out the linear
orbital parameters.
In practice and generically, the sampling obtained by the Simple Monte
Carlo is---in the case of sparse or uninformative data---substantial
and multimodal, or else---in the case of informative
data---insubstantial but unimodal.
In the unimodal case, we follow the Simple Monte Carlo with standard
MCMC to make the sampling more substantial.
% Results
The method produces correct samplings in orbital
parameters for (good) data sets that include as few as three noisy time
points.
We give some examples that show how the posterior probability depends
extremely strongly on the number and time coverage of the
observations.
We discuss extensions to the method that could handle issues with the
noise model or outliers.
\end{abstract}

\keywords{
  binaries: spectroscopic
  ---
  methods: data analysis
  ---
  methods: statistical
  ---
  planets and satellites: fundamental parameters
  ---
  surveys
  ---
  techniques: radial velocities
}

\section{Introduction}

Precise radial-velocity measurements of stars have transformed
astrophysics in the last decades:
They have permitted the discovery of the first planets around other stars,
including especially the unanticipated but common ``hot Jupiters,''
and been used to discover or confirm hundreds
(perhaps thousands?) of planets.
Radial velocity measurements have also been used to find substellar,
degenerate, and black-hole companions to more normal stars, and hold
the promise of delivering the full population statistics for binary
(and trinary) star systems.

With many new stellar spectroscopic surveys operating or under
construction, we expect to have good quality spectra for millions
of stars in the next few years.
Most of these surveys have at least some targets---and many have many
targets---that are observed multiple times.
These surveys can (as an auxilliary or primary goal of their observing
strategies) generate discoveries of planetary, substellar, and stellar
companions.
These discoveries, in turn, will feed population inferences, follow-up
programs, and projects to refine precise stellar models.

However, when radial-velocity observations are not designed with
unambiguous detection and discovery in mind, usually there are
multiple possible binary-star models that are consistent with any
small number of radial-velocity measurements that show stellar
acceleration.
That is, a small number of (even very good) radial velocity
measurements will lead to posterior beliefs about companion orbits and
masses that put substantial plausibility onto multiple qualitatively
different solutions, or (in other words) create a likelihood function
that is highly multi-model in the relevant parameter spaces.
There are currently no safe methods known for exploring these highly
multimodal functions and delivering correct posterior samplings and
reliable probabilistic statements about detection and
characterization.

Here we make an attempt at correcting these problems.
Our approach is to build custom posterior sampling methods that
capitalize on the structure of the binary-star (or star--exoplanet)
kinematics to create provably---or highly probably---correct
samplings.

The structure of the paper is as follows:
We state clearly our assumptions, and demonstrate that we have a
method that is correct under those assumptions.
We perform experiments with the method to understand its properties
and limitations.
We finish by discussing the value of the method, and the changes we
would have to make if we weakened our assumptions, or if we don't
weaken our assumptions but they indeed prove to be far from correct.

\section{Assumptions and method}

In order to set up a well-posed problem and build a path to a
definite solution, we make a set of non-trivial assumptions about the
stellar systems we will use and observations thereof.
\begin{itemize}
\item We assume that we have measurements of the radial velocity of a
  star, and that the time dependence of the expectation of that radial
  velocity is well described by the gravitational orbit of a pair of
  point masses (the two-body problem). We assume that the times are
  (effectively) perfectly known, and in an inertial frame (for
  example, Solar-System barycentric MJD).
\item We assume that each star has exactly one companion, and that the
  radial-velocity measurements are not contaminated by nor affected by
  any other bodies. Since we permit the effective mass of the
  exactly-one companion to go to zero, this assumption is really that
  the star has zero or one companion.
\item We assume that the noise contributions to individual
  radial-velocity measurements are well described as draws from
  zero-mean normal (Gaussian) distributions with correctly known
  variances. We assume that there are no outliers.
\item In addition to all these, we put particular, fairly sensible
  prior probability density functions on all the orbital parameters,
  described below.
\end{itemize}
Each of these assumptions can be challenged, and in particular we
expect some stars to have additional companions, and we expect there
to be outliers and unaccounted sources of noise.
We will return to these assumptions, and the consequences of relaxing
them, in the Discussion Section.

In the two-body celestial mechanics problem (\citealt{Kepler:1609};
and here we are working in a parameterization similar to that of
\citealt{Murray:2010}), and under our above-stated assumptions, the
radial-velocity expectation can be parameterized by six parameters,
which we choose to be period $P$, projected semi-major axis $\asini$,
a phase $\phi_0$ corresponding to a time of pericenter passage, the
eccentricity $e$, an argument of perihelion $\omega$, and a constant
system barycenter radial velocity $v_0$.
The radial velocity $v$ at time $t$ is then given by (see also
equation~63 in \citealt{Murray:2010})
\begin{equation}
  v(t;\theta) = v_0 + \kappa\,[\cos(\omega + f) + e\,\cos\omega]
\end{equation}
where the $\theta$ represents a blob of all the free parameters,
$\kappa$ is the velocity semi-amplitude and $f$ is the true anomaly
given by
\begin{align}
  \kappa = \frac{2\pi\,\asini}{P\,\sqrt{1-e^2}}\\
  \cos f &= \frac{\cos E - e}{1 - e\, \cos E}
\end{align}
and the eccentric anomaly, $E$, must be solved for with the mean
anomaly, $M$,
\begin{align}
  M = \frac{2\pi\, t}{P} - \phi_0\\
  M = E - e\,\sin E \quad .
\end{align}
Of these parameters, four ($P$, $\phi_0$, $e$, $\omega$) are
non-linearly related to the radial-velocity expectation, and two
($\asini$, $v_0$) are linearly related.

Fundamentally, our method is to perform \emph{Simple Monte Carlo} in
the non-linear parameters, and analytically marginalize over the linear
parameters.
The method capitalizes on the following aspects of the problem
structure:
\begin{itemize}
\item There are both linear and non-linear parameters, and we can
  treat them differently; in particular, it is possible to
  analytically marginalize the linear parameters (provided that the
  noise model is pleasant and the prior pdf is conjugate).
\item There is a finite, time-sampling-imposed minimum size or
  resolution---in the period direction---of any features in the
  likelihood function. That is, there cannot be arbitrarily narrow
  modes in the multimodal posterior pdf.
\end{itemize}

Simple Monte Carlo is a method in which the prior pdf is densely
sampled, and then many of the samples are rejected by a
rejection-sampling step that uses the likelihood as the rejection
scalar.
The rejection step works as follows:
\begin{enumerate}
\item For each sample $k$ in the prior pdf sampling there is a
  likelihood value $L_k$ (a probability for the data given the
  parameters, not logarithmic).
\item There is a maximum value $L_{\rm max}$ that is the largest value of
  $L_k$ found across all of the samples in the entire sampling.
\item For each sample $k$, choose a random number $r_k$ between 0 and
  $L_{\rm max}$
\item Reject the sample $k$ if $L_k < r_k$.
\item The number of samples that survive the rejection is (hereafter) $M$.
\end{enumerate}
Note that this algorithm is guaranteed to produce at least one
surviving sample; of course if only one sample survives (or any very
small number), the sampling will not guaranteed to be fair.
That said, if the original sampling of the prior pdf is dense enough
that many survive the rejection step, the surviving samples do
constitute a fair sampling from the posterior pdf.

Our prior pdf in the non-linear parameters is very simple:
\begin{align}
    p(\ln P) &= \mathcal{U}(\ln P_{\rm min}, \ln P_{\rm max})\\
    p(\omega) &= \mathcal{U}(0, 2\pi) ~ [{\rm rad}]\\
    p(\phi_0) &= \mathcal{U}(0, 2\pi) ~ [{\rm rad}]\\
    p(e) &= {\rm Beta}(a, b) = \frac{\Gamma(a+b)}{\Gamma(a) \, \Gamma(b)} \, e^{a-1} \, [1 - e]^{b-1}
\end{align}
where $\mathcal{U}(x_1, x_2)$ is the uniform distribution over the
domain $(x_1, x_2)$ and the prior over eccentricity is the Beta
distribution with $a=0.867$, $b=3.03$ \citep{Kipping:2013}.
In different experiments we use different values for hyperparameters
$P_{\rm min}$ and $P_{\rm max}$; we are explicit about our choices in
each of the experiments described below.
We sample the prior pdf directly and explicitly with standard
\project{numpy.random} calls (\citealt{numpy}).
In practice we usually take $2^{28}$ (that's about a quarter billion)
samples in the prior-pdf sampling. Holy fuck that's a lot!

The unmarginalized likelihood function $L$ is
\begin{equation}
\ln L = -\frac{1}{2}\,\sum_{n=1}^N \frac{[v_n - v(t_n;\theta)]^2}{\sigma_n^2}
\end{equation}
where $n$ indexes the individual data points $v_n$, $v(t)$ is the
radial velocity prediction at time $t$ given the parameters $\theta$, the
data-point times are the $t_n$, and the $\sigma_n^2$ is the (presumed
correctly known) Gaussian noise variance for data point $n$.
Note that the form of this likelihood function is set entirely by
the assumptions, given above.

We rejection-sample, however, using a marginalized likelihood, where
we analytically marginalize out the linear parameters ($\asini$,
$v_0$).
We construct a $N\times 2$ design matrix consisting of a column of
unit-$[\asini]$ predictions (given the non-linear parameters) and a
column of ones.
We perform standard linear least-square fitting with this design
matrix to obtain the best-fit values for the two linear parameters,
and the standard $2\times 2$ linear-fitting covariance matrix $C_k$ for their
uncertainties.
With these, we can construct---for each prior sample---the marginalized likelihood $Q_k$
\begin{equation}
\ln Q_k = -\frac{1}{2}\,\sum_{n=1}^N \frac{[v_n - v(t_n;\theta_k)]^2}{\sigma_n^2} -\frac{1}{2}\,\ln ||C_k||
\end{equation}
where the prediction $v(t_n;\theta_k)$ is taken at the best-fit values
of the linear parameters given sample $k$ of the non-linear
parameters, and the log-determinant term accounts for the volume in
the marginalization integral.
The marginalization assumes that the prior pdfs on the linear parameters
are very broad and Gaussian---or
at least very flat over the range of relevance---and that they do not depend
on the nonlinear parameters in any way (which is a substantial restriction;
see the Discussion).
These $Q_k$ are used in the rejection sampling algorithm described above
as Simple Monte Carlo.

There are three possible outcomes at the end of the rejection
sampling, based on two thresholds:
We set a minimum number of samples $M_{\rm min}=128$.
We also set a period resolution $\Delta = [4\,P^2] / [2\pi\,T]$, (with
$P$ set to the median period across the surviving samples and $T$ set to
the time span of the data; max minus min).
This $\Delta$ is an expansion of the period
resolution expected from an information-theory (sampling theorems) perspective.
The three possible outcomes are:
\begin{itemize}
\item $M\geq M_{\rm min}$ samples survive the rejection.
  In this case, we are done.
\item $M<M_{\rm min}$ samples survive the rejection, and these samples
  have a root-variance (rms) in the period parameter $P$ that is
  smaller than $\Delta$.
  In this case, we use the surviving samples (or sample) to
  initialize a MCMC sampling using the \emcee\ package (\citealt{emcee}).
  We sample the
  prior densely enough initially that if only a few samples survive,
  we are fairly confident that the data are informative enough that
  the posterior pdf is approximately unimodal for our purposes. That's
  an assumption!
\item $M<M_{\rm min}$ samples survive the rejection, and these samples
  span a period range larger than $\Delta$.
  In this case, we iterate the Simple
  Monte Carlo, starting from new prior-pdf samplings, concatenating
  the surviving samples, until the full set of surviving samples is
  larger than $M_{\rm min}$. This is expensive!
\end{itemize}
When we trigger the intialization and operation of \emcee, we do the following:
\begin{enumerate}
\item Randomly generate $M_{\rm min}$ sets of parameters $\theta_m$
  (linear and nonlinear parameters) in a small, Gaussian ball around
  the highest-$Q_k$ sample from the Simple Monte Carlo.
\item Run \emcee\ on this ensemble as per the instructions written on
  the side of the \emcee\ packaging (\citealt{emcee}) for $2^{16}$ steps.
\item Take the final state of the $M_{\rm min}$-element ensemble as a
  fair sampling of the posterior pdf.
\end{enumerate}
This procedure ensures that no matter what path we take, we end up with
at least $M_{\rm min}$ samples from the posterior for any input data.

\section{Experiments and results}

Introduce the \apogee\ data
(\citealt{Majewski:2015,Holtzman:2015,Eisenstein:2011,Alam:2015}), and
our poster-child (\citealt{Troup:2016}).

Show results as a function of $N$.

Show switch-over from SMC to \emcee.

\section{Discussion}

What did we find?

What can this system be used for? Discovery, characterization, and
hierarchial inference.

HOGG The assumption of single companion has the following issues...

HOGG The assumption that the linear-parameter prior doesn't depend
on the nonlinear parameters is absurd! Why?

Of the assumptions, probably the most severe is the Gaussian noise
with known variance. \textbf{This is wrong, very wrong!} Ideas about
ameliorating this, and what those options would cost us. Advice for
those using the system despite these issues.

\acknowledgements
It is a pleasure to thank
  Ben Weaver (NOAO),
for valuable discussions.
This research was partially supported by [many grants].
This project was started at AstroHackWeek 2016, organized by Kyle
Barbary (UCB) and Phil Marshall (SLAC) at the Berkeley Institute for
Data Science.

The code used in this project is available from
\url{https://github.com/adrn/thejoker} under the MIT open-source
software license. This version of the paper was generated with git
commit \texttt{\githash~(\gitdate)}.

\bibliographystyle{apj}
\bibliography{thejoker}

\end{document}
